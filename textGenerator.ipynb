{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "name": "textGenerator.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RJu75BPGmd8m",
        "outputId": "d73bd59f-40bd-49b0-be14-ebef13ca17c7"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np \n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from sklearn.model_selection import train_test_split\n",
        "import nltk\n",
        "import re\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')\n",
        "from nltk.tokenize import word_tokenize\n",
        "from sklearn.metrics import accuracy_score\n",
        "import time\n",
        "device=torch.device('cuda ' if torch.cuda.is_available() else 'cpu')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t_idalsonwwj",
        "outputId": "f701efa3-a1cb-4f24-efe0-65fa4692949b"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_gsP6KoKms6z"
      },
      "source": [
        "\n",
        "\n",
        "def preprocessing(word):\n",
        "   # replace digits with no space\n",
        "     word = re.sub(r\"\\d\", '', word)\n",
        "   # Replace all runs of whitespaces with no space\n",
        "     word = re.sub(r\"\\s+\", '', word)\n",
        "     return word\n",
        "\n",
        "lines=[]\n",
        "with open('/content/drive/MyDrive/alice.txt','r') as f:\n",
        "    for line in f:\n",
        "        line=line.lower().split()\n",
        "    \n",
        "        for i in line:\n",
        "          i=re.findall(r\"[\\w]+|[.,!?;]\", i) #we separate punctuation marks from words\n",
        "          lines.append(i)\n",
        "\n",
        "text=np.concatenate(np.array(lines,dtype=object))"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vUv7ZQ75mtDQ"
      },
      "source": [
        "for i,s in enumerate(text):\n",
        "    text[i]=preprocessing(s)"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9AIM5uBCHsny"
      },
      "source": [
        "word_list=[]\n",
        "for i in text:\n",
        "    if i not in word_list:\n",
        "        word_list.append(i)\n",
        "        \n",
        "one_hot_dict={w:i+1 for i,w in enumerate(word_list) }"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "keo0ZiuqgtXU"
      },
      "source": [
        "for i,s in enumerate(text):\n",
        "  text[i]=one_hot_dict[s]"
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-S9aTtqdM2qo",
        "outputId": "5d5a1a72-30e9-4d6a-81ef-6727613c7cde"
      },
      "source": [
        "text[:300] #text list preview"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12',\n",
              "       '13', '14', '15', '16', '17', '18', '19', '20', '5', '21', '22',\n",
              "       '23', '15', '24', '25', '11', '26', '27', '28', '29', '30', '31',\n",
              "       '32', '33', '5', '34', '18', '19', '9', '35', '22', '36', '37',\n",
              "       '31', '38', '39', '28', '40', '41', '37', '22', '23', '42', '43',\n",
              "       '5', '44', '15', '45', '34', '22', '46', '8', '47', '39', '28',\n",
              "       '40', '48', '49', '30', '9', '50', '41', '18', '51', '52', '53',\n",
              "       '54', '53', '30', '55', '22', '56', '5', '57', '58', '59', '18',\n",
              "       '60', '13', '61', '23', '62', '22', '63', '5', '64', '15', '65',\n",
              "       '45', '66', '67', '68', '69', '70', '5', '71', '15', '72', '73',\n",
              "       '23', '74', '5', '75', '22', '76', '77', '45', '78', '6', '79',\n",
              "       '80', '81', '82', '83', '17', '18', '3', '84', '9', '25', '49',\n",
              "       '13', '85', '41', '86', '87', '88', '89', '8', '90', '37', '49',\n",
              "       '13', '91', '92', '15', '5', '93', '11', '94', '5', '6', '95',\n",
              "       '11', '96', '22', '97', '98', '99', '97', '98', '99', '2', '100',\n",
              "       '69', '101', '99', '76', '30', '46', '37', '102', '103', '22',\n",
              "       '37', '104', '11', '18', '86', '30', '105', '11', '106', '107',\n",
              "       '108', '109', '22', '36', '108', '5', '110', '37', '111', '112',\n",
              "       '113', '114', '87', '36', '76', '5', '6', '115', '116', '45',\n",
              "       '117', '92', '15', '118', '119', '120', '22', '23', '121', '108',\n",
              "       '37', '22', '23', '122', '123', '20', '22', '8', '124', '11', '18',\n",
              "       '125', '22', '56', '37', '126', '127', '18', '52', '86', '30',\n",
              "       '31', '128', '129', '130', '45', '6', '79', '131', '45', '119',\n",
              "       '120', '22', '28', '45', '117', '11', '132', '92', '15', '37',\n",
              "       '22', '23', '133', '79', '134', '22', '30', '82', '127', '5',\n",
              "       '135', '136', '37', '22', '23', '137', '9', '138', '41', '110',\n",
              "       '11', '139', '37', '140', '4', '45', '141', '6', '7', '142', '5',\n",
              "       '143', '3', '41', '144', '145', '4', '146', '8', '136', '37', '22',\n",
              "       '128'], dtype='<U14')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3A0wrxTcmtKo"
      },
      "source": [
        "#model hyper parameters\n",
        "batch_size=32\n",
        "timestep=30 #each time steps occuring 30 words len\n",
        "\n",
        "vocab_size=len(one_hot_dict)+1 #extra 1 for padding\n",
        "embed_size=128.  #Input features to the LSTM\n",
        "hidden_size=512  #Number of LSTM units\n",
        "\n",
        "rep_tensor=torch.LongTensor(text.astype('int')) #text list converting to  tensor \n",
        "\n",
        "num_batch=rep_tensor.shape[0]//batch_size #net number of batches\n",
        "\n",
        "rep_tensor=rep_tensor[:num_batch*batch_size] #rep tensor \n",
        "\n",
        "rep_tensor=rep_tensor.view(32,-1) "
      ],
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QZ9dIM84Oi4_",
        "outputId": "37c98b42-f0f8-460a-e4e9-7389ad473bc2"
      },
      "source": [
        "num_batches=rep_tensor.shape[1]//timestep\n",
        "print(num_batches)"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "32\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hGOB1sUvOc97",
        "outputId": "5e1bb97c-35db-44d0-bb36-1dd96fcf6c12"
      },
      "source": [
        "rep_tensor.shape"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([32, 986])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1JSbbjWVOUtx",
        "outputId": "c442b099-0a59-4c44-9dc8-05bcef6f5c9f"
      },
      "source": [
        "rep_tensor #preview"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[   1,    2,    3,  ...,   23,  146,   20],\n",
              "        [ 337,   11,  158,  ...,    8,   11,  158],\n",
              "        [  22,   41,  324,  ...,   99,   97,   99],\n",
              "        ...,\n",
              "        [   5, 1210,   22,  ...,   87,   36,   37],\n",
              "        [ 705,  217,  343,  ...,  225,   86,  259],\n",
              "        [   5, 1093,   22,  ...,  147,   30,   68]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FiJkzVKRmtPw"
      },
      "source": [
        "class textGenerator(nn.Module):\n",
        "    def __init__(self,vocab_size,embed_size,hidden_size):\n",
        "        super(textGenerator,self).__init__()\n",
        "\n",
        "        self.embed=nn.Embedding(vocab_size,embed_size)\n",
        "\n",
        "        self.fc=nn.Linear(hidden_size,vocab_size)\n",
        "\n",
        "        self.lstm=nn.LSTM(input_size=embed_size,\n",
        "                          hidden_size=hidden_size,\n",
        "                          num_layers=1,\n",
        "                          batch_first=True)\n",
        "        \n",
        "        self.drop=nn.Dropout(0.3)\n",
        "\n",
        "        self.relu=nn.ReLU()\n",
        "\n",
        "    \n",
        "    def forward(self,x):\n",
        "        input=x.clone()\n",
        "        \n",
        "        # Perform Word Embedding \n",
        "        x=self.embed(x)\n",
        "        #x = x.view(batch_size,timesteps,embed_size)\n",
        "        x,_=self.lstm(x)\n",
        "        # (batch_size*timesteps, hidden_size)\n",
        "        x=x.contiguous().view(-1, hidden_size)\n",
        "        x=self.drop(x)\n",
        "        out=self.fc(x)\n",
        "        #Decode hidden states of all time steps\n",
        "        out=out.view(input.shape[0],input.shape[1],vocab_size)\n",
        "        \n",
        "        return out[:,-1]\n",
        "    \n",
        "    \n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 375
        },
        "id": "FjBzT4i4mtSi",
        "outputId": "7676f409-b73b-4b36-8cc5-402a3c71ff9f"
      },
      "source": [
        "\n",
        "model=textGenerator(vocab_size,embed_size,hidden_size)\n",
        "optimizer=torch.optim.Adam(model.parameters())\n",
        "criterion=nn.CrossEntropyLoss()\n",
        "\n",
        "train_loss=[]\n",
        "train_l=0\n",
        "\n",
        "model.train()\n",
        "\n",
        "for epoch in range(10):\n",
        "    t0 = time.time()\n",
        "\n",
        "    for i in range(0 ,rep_tensor.shape[1]-timestep):\n",
        "    \n",
        "        inputs=rep_tensor[:,i:i+timestep]\n",
        "        labels=rep_tensor[:,i+timestep]\n",
        "\n",
        "        outputs=model(inputs)\n",
        "        loss = criterion(outputs, labels.reshape(-1))\n",
        "        \n",
        "        train_l+=loss\n",
        "        \n",
        "        model.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        acc=accuracy_score(torch.argmax(outputs,dim=1),labels)\n",
        "        \n",
        "    print('epoch {:} acc {:},  seconds {:.2f}'.format(epoch,acc,(time.time() - t0)/60))\n",
        "        \n",
        "        \n",
        "\n",
        "    \n",
        "    "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-21-9affb9e40ad7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0macc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maccuracy_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    243\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    244\u001b[0m                 inputs=inputs)\n\u001b[0;32m--> 245\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    246\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    247\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    145\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m    146\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 147\u001b[0;31m         allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    148\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FvtS24ylnKTQ"
      },
      "source": [
        "torch.save(model.state_dict(), '/content/drive/MyDrive/textGen.pt') #save to model "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uMqMlTbkNseR",
        "outputId": "8771908e-af35-49e5-b458-63e6aa0103e3"
      },
      "source": [
        "model.load_state_dict(torch.load('/content/drive/MyDrive/textGen.pt'))#load to model \n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zNaoCt4ZhCxX",
        "outputId": "cadcd1af-e3ed-4fbf-9865-5858f2fea01b"
      },
      "source": [
        "text_n=np.concatenate(np.array(lines[-10:],dtype=object))\n",
        "for i,s in  enumerate(text_n):\n",
        "    text_n[i]=one_hot_dict[preprocessing(s)]\n",
        "\n",
        "text_n"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['18', '51', '575', '608', '22', '23', '5', '2573', '2393', '1637',\n",
              "       '3', '5', '228'], dtype='<U6')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jcVLcF4fm-Nc"
      },
      "source": [
        "model.eval()\n",
        "with torch.no_grad():\n",
        "\n",
        "  with open('/content/drive/MyDrive/result.txt','w'):\n",
        "\n",
        "    input=torch.tensor(text_n.astype('int').reshape(1,-1))\n",
        "    for i in range(1000):\n",
        "\n",
        "        output=model(input)\n",
        "\n",
        "        output_item=torch.argmax(output,dim=1)\n",
        "\n",
        "        output=torch.cat((input.reshape(1,-1),output_item.reshape(1,-1) ),1).reshape(1,-1)\n",
        "\n",
        "        input=output.clone()\n",
        "\n",
        "    listt=[]\n",
        "    for i in output[0]:\n",
        "      listt.append(word_list[i-1]+' ')\n",
        "    end=' '.join(listt)\n"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 562
        },
        "id": "Y_AudohLGoUL",
        "outputId": "b1c1e644-3766-4e21-8402-2977c3c9a9c1"
      },
      "source": [
        "end"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'her  own  child  life  ,  and  the  happy  summer  days  .  the  end  of  the  party  went  back  to  the  game  .  chapter  ix  .  the  mock  turtle  s  story  you  can  t  think  how  glad  i  am  to  go  in  ?  she  went  on  ,  what  am  i  to  do  with  this  creature  when  i  get  it  home  ?  when  it  grunted  again  ,  so  violently  ,  that  she  looked  down  into  its  face  in  some  alarm  .  this  time  there  could  be  no  sort  of  chance  of  her  ever  getting  out  of  the  room  again  ,  no  wonder  she  felt  unhappy  .  it  was  much  pleasanter  at  home  ,  thought  poor  alice  ,  when  one  wasn  t  the  look  of  the  lobster  quadrille  ?  it  s  the  most  curious  thing  ,  and  alice  was  too  much  frightened  to  say  a  word  ,  but  she  did  not  venture  to  go  near  the  house  till  she  had  brought  herself  down  to  nine  inches  high  .  chapter  vi  .  pig  and  pepper  for  a  minute  or  two  she  stood  looking  at  the  house  ,  and  wondering  what  to  do  next  ,  and  in  despair  she  put  her  hand  in  her  pocket  ,  and  was  delighted  to  find  that  her  neck  would  bend  about  easily  in  any  direction  ,  like  a  serpent  .  she  had  just  succeeded  in  curving  it  down  into  a  graceful  altogether  .  alice  did  not  venture  to  go  with  its  face  down  to  its  feet  ,  and  she  hastily  dried  her  eyes  to  see  what  was  coming  .  it  was  the  white  rabbit  returning  ,  splendidly  dressed  ,  with  a  pair  of  white  kid  gloves  in  one  hand  and  a  large  fan  in  the  other  he  came  trotting  along  in  a  great  hurry  ,  muttering  to  himself  as  he  came  ,  oh  !  the  duchess  ,  the  duchess  !  oh  !  the  duchess  !  the  knave  shook  ,  who  was  gone  to  the  shore  ,  you  know  .  it  was  high  time  to  go  ,  for  the  pool  was  getting  quite  crowded  with  the  other  ,  and  then  sat  upon  it  .  i  m  glad  i  ve  seen  that  done  ,  thought  alice  .  i  ve  so  often  read  in  the  newspapers  ,  at  the  end  of  trials  ,  there  was  some  attempts  at  applause  ,  which  was  immediately  suppressed  by  the  officers  of  the  court  ,  and  i  never  understood  what  it  meant  till  now  .  if  that  s  all  you  know  about  it  ,  you  may  stand  down  ,  continued  the  king  .  i  can  t  go  no  lower  ,  said  the  hatter  i  m  a  deal  too  flustered  to  tell  you  !  i  shall  be  a  great  deal  ,  ma  !  said  the  duchess  ,  as  he  shook  his  grey  locks  ,  i  feared  it  might  injure  the  brain  ;  but  ,  now  that  i  m  perfectly  sure  i  have  none  ,  why  ,  i  do  it  again  and  again  .  you  are  old  ,  said  the  youth  ,  as  i  mentioned  before  ,  and  then  i  ll  tell  you  my  history  ,  you  know  .  it  s  high  time  to  be  ,  said  alice  ,  seriously  ,  i  ll  have  nothing  more  to  do  with  you  .  mind  now  !  the  poor  little  thing  sobbed  again  or  grunted  ,  it  was  impossible  to  say  which  ,  and  they  went  on  for  some  while  in  silence  .  alice  was  just  beginning  to  think  to  herself  ,  now  ,  what  am  i  to  do  with  this  creature  when  i  get  it  home  ?  when  it  grunted  again  ,  so  violently  ,  that  she  looked  down  into  its  face  in  some  alarm  .  this  time  there  could  be  no  sort  of  chance  of  her  ever  getting  out  of  the  room  again  ,  no  wonder  she  felt  unhappy  .  it  was  much  pleasanter  at  home  ,  thought  poor  alice  ,  when  one  wasn  t  the  look  of  the  lobster  quadrille  ?  it  s  the  most  curious  thing  ,  and  alice  was  too  much  frightened  to  say  a  word  ,  but  she  did  not  venture  to  go  near  the  house  till  she  had  brought  herself  down  to  nine  inches  high  .  chapter  vi  .  pig  and  pepper  for  a  minute  or  two  she  stood  looking  at  the  house  ,  and  wondering  what  to  do  next  ,  and  in  despair  she  put  her  hand  in  her  pocket  ,  and  was  delighted  to  find  that  her  neck  would  bend  about  easily  in  any  direction  ,  like  a  serpent  .  she  had  just  succeeded  in  curving  it  down  into  a  graceful  altogether  .  alice  did  not  venture  to  go  with  its  face  down  to  its  feet  ,  and  she  hastily  dried  her  eyes  to  see  what  was  coming  .  it  was  the  white  rabbit  returning  ,  splendidly  dressed  ,  with  a  pair  of  white  kid  gloves  in  one  hand  and  a  large  fan  in  the  other  he  came  trotting  along  in  a  great  hurry  ,  muttering  to  himself  as  he  came  ,  oh  !  the  duchess  ,  the  duchess  !  oh  !  the  duchess  !  the  knave  shook  ,  who  was  gone  to  the  shore  ,  you  know  .  it  was  high  time  to  go  ,  for  the  pool  was  getting  quite  crowded  with  the  other  ,  and  then  sat  upon  it  .  i  m  glad  i  ve  seen  that  done  ,  thought  alice  .  i  ve  so  often  read  in  the  newspapers  ,  at  the  end  of  trials  ,  there  was  some  attempts  at  applause  ,  which  was  immediately  suppressed  by  the  officers  of  the  court  ,  and  i  never  understood  what  it  meant  till  now  .  if  that  s  all  you  know  about  it  ,  you  may  stand  down  ,  continued  the  king  .  i  can  t  go  no  lower  ,  said  the  hatter  i  m  a  deal  too  flustered  to  tell  you  !  i  shall  be  a  great  deal  ,  ma  !  said  the  duchess  ,  as  he  shook  his  grey  locks  ,  i  feared  it  might  injure  the  brain  ;  but '"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fI-K2ZsVk6Sn",
        "outputId": "1de57130-c171-49d1-8a42-594a75522172"
      },
      "source": [
        "len(end.split())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "524"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 348
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 120
        },
        "id": "iZFqJJAja1h9",
        "outputId": "6936ffbb-ebf5-4736-e37c-18c886d78f2c"
      },
      "source": [
        "end\n"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'and  find  a  pleasure  in  all  their  simple  joys  ,  remembering  her  own  child  life  ,  and  the  happy  summer  days  .  the  end  of  the  baby  ,  and  shut  his  note  book  hastily  .  consider  your  verdict  ,  he  said  to  the  jury  ,  in  a  very  humble  tone  ,  going  down  on  one  knee  as  he  spoke  ,  we  were  trying  i  see  !  said  the  queen  ,  who  had  meanwhile  been  examining  the  roses  .  off  with  their  heads  !  and  the  procession  moved  on  ,  three  of  the  soldiers  remaining  behind  to  execute  the  unfortunate  gardeners  ,  who  ran  to  alice  for  protection  .  you  shan  t  be  beheaded  !  said  alice  ,  and  she  put  them  into  a  large  flower  pot  that  stood  near  .  the  three  soldiers  wandered  ,  and  the  dormouse  began  in  a  low  ,  weak  voice  .  now  ,  i  give  you  fair  warning  ,  shouted  the  queen  ,  stamping  on  the  ground  as  she  spoke  fancy  curtseying  as  you  re  falling  through  the  air  !  do  you  think  you  could  manage  the  best  ?  i  can  t  take  this  .  you  d  better  not  be  !  said  alice  .  in  a  very  tone  though  ,  i  won  t  have  any  pepper  in  my  kitchen  at  all  .  soup  does  very  well  without  maybe  it  s  always  pepper  that  makes  people  hot  tempered  ,  she  went  on  ,  what  am  i  to  do  with  this  creature  when  i  get  it  home  ?  when  it  grunted  again  ,  so  violently  ,  that  she  looked  down  into  its  face  in  some  alarm  .  this  time  there  could  be  no  sort  of  chance  of  her  ever  getting  out  of  the  room  again  ,  no  wonder  she  felt  unhappy  .  it  was  much  pleasanter  at  home  ,  thought  poor  alice  ,  when  one  wasn  t  the  look  of  the  lobster  quadrille  ?  it  s  the  most  curious  thing  ,  and  alice  was  too  much  frightened  to  say  a  word  ,  but  she  did  not  venture  to  go  near  the  house  till  she  had  brought  herself  down  to  nine  inches  high  .  chapter  vi  .  pig  and  pepper  for  a  minute  or  two  she  stood  looking  at  the  house  ,  and  wondering  what  to  do  next  ,  and  in  despair  she  put  her  hand  in  her  pocket  ,  and  was  delighted  to  find  that  her  neck  would  bend  about  easily  in  any  direction  ,  like  a  serpent  .  she  had  just  succeeded  in  curving  it  down  into  a  graceful  altogether  .  alice  did  not  venture  to  go  with  its  face  down  to  its  feet  ,  and  she  hastily  dried  her  eyes  to  see  what  was  coming  .  it  was  the  white  rabbit  returning  ,  splendidly  dressed  ,  with  a  pair  of  white  kid  gloves  in  one  hand  and  a  large  fan  in  the  other  he  came  trotting  along  in  a  great  hurry  ,  muttering  to  himself  as  he  came  ,  oh  !  the  duchess  ,  the  duchess  !  oh  !  the  duchess  !  the  knave  shook  ,  who  was  gone  to  the  shore  ,  you  know  .  it  was  high  time  to  go  ,  for  the  pool  was  getting  quite  crowded  with  the  other  ,  and  then  sat  upon  it  .  i  m  glad  i  ve  seen  that  done  ,  thought  alice  .  i  ve  so  often  read  in  the  newspapers  ,  at  the  end  of  trials  ,  there  was  some  attempts  at  applause  ,  which  was  immediately  suppressed  by  the  officers  of  the  court  ,  and  i  never  understood  what  it  meant  till  now  .  if  that  s  all  you  know  about  it  ,  you  may  stand  down  ,  continued  the  king  .  i  can  t  go  no  lower  ,  said  the  hatter  i  m  a  deal  too  flustered  to  tell  you  !  i  shall  be  a  great  deal  ,  ma  !  said  the  duchess  ,  as  he  shook  his  grey  locks  ,  i  feared  it  might  injure  the  brain  ;  but  ,  now  that  i  m  perfectly  sure  i  have  none  ,  why  ,  i  do  it  again  and  again  .  you  are  old  ,  said  the  youth  ,  as  i  mentioned  before  ,  and  then  i  ll  tell  you  my  history  ,  you  know  .  it  s  high  time  to  be  ,  said  alice  ,  seriously  ,  i  ll  have  nothing  more  to  do  with  you  .  mind  now  !  the  poor  little  thing  sobbed  again  or  grunted  ,  it  was  impossible  to  say  which  ,  and  they  went  on  for  some  while  in  silence  .  alice  was  just  beginning  to  think  to  herself  ,  now  ,  what  am  i  to  do  with  this  creature  when  i  get  it  home  ?  when  it  grunted  again  ,  so  violently  ,  that  she  looked  down  into  its  face  in  some  alarm  .  this  time  there  could  be  no  sort  of  chance  of  her  ever  getting  out  of  the  room  again  ,  no  wonder  she  felt  unhappy  .  it  was  much  pleasanter  at  home  ,  thought  poor  alice  ,  when  one  wasn  t  the  look  of  the  lobster  quadrille  ?  it  s  the  most  curious  thing  ,  and  alice  was  too  much  frightened  to  say  a  word  ,  but  she  did  not  venture  to  go  near  the  house  till  she  had  brought  herself  down  to  nine  inches  high  .  chapter  vi  .  pig  and  pepper  for  a  minute  or  two  she  stood  looking  at  the  house  ,  and  wondering  what  to  do  next  ,  and  in  despair  she  put  her  hand  in  her  pocket  ,  and  was  delighted  to  find  that  her  neck  would  bend  about  easily  in  any  direction  ,  like  a  serpent  .  she  had  just  succeeded  in  curving  it  down  into  a  graceful  altogether  .  alice  did  not  venture  to  go  with  its  face  down  to  its  feet  ,  and  she  hastily  dried  her  eyes  to  see  what  was  coming  .  it  was  the  white  rabbit  returning  ,  splendidly  dressed  ,  with  a  pair  of  white  kid  gloves  in  one  hand  and  a  large  fan  in  the  other  he  came  trotting  along  in  a  great  hurry  , '"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 562
        },
        "id": "L6kd7JPBexg4",
        "outputId": "980ba02c-2833-477a-904e-a47c3c12aec5"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'failure  thought  snatch  think  no  cant  tunnel  no  considering  shoes  it  happened  sort  seemed  sister  question  air  altogether  get  once  no  learnt  mabel  worth  ropewill  one  seemed  made  down  wondered  measure  having  show  learnt  care  much  home  ending  having  is  mice  learnt  pleaded  aloud  rabbithole  arms  was  quietly  sister  lap  having  through  get  duchess  what  rabbithole  curiouser  angry  having  rabbithole  savage  occasionally  alternately  having  rabbithole  iii  cried  adding  no  be  worth  seemed  what  end  ache  notebook  well  remarkable  beginning  thought  escape  frightened  aloud  was  thought  girl  altered  hot  finger  others  learnt  pop  world  time  sure  rabbithole  doubtful  dried  aloud  having  how  bank  get  duchess  what  end  ache  notebook  well  off  trotting  filled  every  likely  ask  hoarse  climb  dipped  sitting  rabbithole  judge  dates  no  the  driest  having  dipped  respectful  was  for  pop  time  well  had  beginning  centre  left  get  wonder  afterwards  heads  indignant  it  no  cant  felt  suddenly  no  trotting  how  bank  cardboard  way  waistcoatpocket  measure  down  for  wondered  side  night  twice  learnt  itself  no  straight  down  care  please  pink  rabbithole  turtles  company  crossly  what  thought  solemnly  angry  was  then  the  this  name  pink  thought  under  lizard  asleep  having  rabbithole  fountains  head  i  queens  thought  blow  line  get  used  found  one  till  fall  thought  paris  shelves  beginning  cake  bank  rabbithole  effect  well  had  for  shrinking  stopping  rabbithole  screaming  having  centre  rabbithole  girl  walked  peeped  completely  mary  rabbithole  frogfootman  having  coming  way  get  once  time  trotting  peeped  thought  girl  seen  had  peeped  feel  way  rabbithole  however  manage  waistcoatpocket  who  having  right  seemed  rabbithole  croquet  while  get  sister  end  driest  showing  rabbithole  turtles  company  crossly  having  hurried  rabbithole  sun  having  hurried  tumbled  pegs  no  half  rabbithole  sun  execution  what  finish  majesty  no  beginning  what  seen  had  aloud  time  had  they  nor  changed  sitting  rabbithole  themi  having  thought  girl  certainly  bank  no  likely  neck  beginning  about  one  saw  get  pop  no  dreadfully  picking  tunnel  thought  only  having  rabbithole  cook  pink  end  driest  manage  mice  learnt  before  egg  maam  finger  pack  rabbithole  crimson  sitting  consultation  rabbithole  lory  crossly  having  aloud  rabbithole  sun  mouses  learnt  wondered  an  bank  having  aloud  hatter  waistcoatpocket  doorway  use  considering  down  dinah  much  cat  be  worth  we  straight  down  going  off  down  late  pet  get  them  rabbithole  matter  no  use  way  sitting  rabbithole  turtles  company  it  this  sulky  having  hurried  no  cried  the  waistcoatpocket  end  curled  hot  dipped  all  so  rumbling  rabbithole  savage  watch  rabbithole  its  having  bank  rabbithole  effect  well  no  beginning  natural  well  out  one  rabbithole  arms  quite  get  wondered  pictures  lessons  sitting  whats  what  no  having  show  no  beginning  neck  opening  down  care  are  rabbithole  turtles  company  crossly  having  world  itll  seem  among  get  an  considering  were  answer  your  no  dark  out  what  thought  older  angry  having  rabbithole  trying  grunt  shepherd  pink  rabbithole  turtles  company  having  world  mice  half  rabbithole  turtles  company  salmon  affectionately  cleared  having  considering  bank  hurried  suddenly  annoyed  jellyfish  seemed  rabbithole  advance  way  sitting  rabbithole  hear  nor  gave  twentyfour  dipped  all  yawning  rabbithole  judge  seemed  dates  the  bank  rabbithole  effect  having  thought  boots  sitting  rabbit  gloves '"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QGD06VRGm-SU"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}